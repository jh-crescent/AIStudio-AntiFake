{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import gc\r\n",
    "import datetime\r\n",
    "import time\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import pickle\r\n",
    "\r\n",
    "import warnings \r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.optimizer as optim\r\n",
    "from paddle.io import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 超参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = \"data/data79052/\"\r\n",
    "DATA_PATH = \"work/data/\"\r\n",
    "BATCH_SIZE = 256  # 128\r\n",
    "USE_ALL_DATA = False\r\n",
    "USE_TIME = True  # 是否使用时间特征\r\n",
    "\r\n",
    "EMB_DIM = 32  # 嵌入维度  16\r\n",
    "REDUCE_RATIO = 2  # 先扩大一倍再降回来\r\n",
    "MB_NUMS = 3  # MaskBlocks数量  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xDeepFM相关参数\r\n",
    "CIN_EMB_DIM = 32  # 32\r\n",
    "CIN_H_0 = 22  # 原始特征数量\r\n",
    "CIN_HK_LIST = [256, 128, 64]  # [100, 100, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, use_uint=True, verbose=True):\r\n",
    "    \"\"\"\r\n",
    "    节约内存函数\r\n",
    "    :param df: 原始从文件中读取出来的df数据\r\n",
    "    :param use_uint: 是否使用无符号整型处理数据\r\n",
    "    :param verbose: 是否打印输出处理前后内存占用情况\r\n",
    "    :return: 处理后的df\r\n",
    "    \"\"\"\r\n",
    "    numerics = ['uint8', 'uint16', 'uint32', 'uint64',\r\n",
    "                'int8', 'int16', 'int32', 'int64', \r\n",
    "                'float16', 'float32', 'float64']\r\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \r\n",
    "    for col in df.columns:\r\n",
    "        col_type = df[col].dtypes\r\n",
    "        if col_type in numerics:\r\n",
    "            c_min = df[col].min()\r\n",
    "            c_max = df[col].max()\r\n",
    "            if 'int' in str(col_type):\r\n",
    "                if use_uint and c_min >= 0:  # uint类型\r\n",
    "                    if c_max <= np.iinfo(np.uint8).max:\r\n",
    "                        df[col] = df[col].astype(np.uint8)\r\n",
    "                    elif c_max <= np.iinfo(np.uint16).max:\r\n",
    "                        df[col] = df[col].astype(np.uint16)\r\n",
    "                    elif c_max <= np.iinfo(np.uint32).max:\r\n",
    "                        df[col] = df[col].astype(np.uint32)\r\n",
    "                    else:\r\n",
    "                        df[col] = df[col].astype(np.uint64)\r\n",
    "                else:  # signed int类型\r\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\r\n",
    "                        df[col] = df[col].astype(np.int8)\r\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\r\n",
    "                        df[col] = df[col].astype(np.int16)\r\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\r\n",
    "                        df[col] = df[col].astype(np.int32)\r\n",
    "                    else:\r\n",
    "                        df[col] = df[col].astype(np.int64)  \r\n",
    "            else:  # float类型\r\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\r\n",
    "                    df[col] = df[col].astype(np.float16)\r\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\r\n",
    "                    df[col] = df[col].astype(np.float32)\r\n",
    "                else:\r\n",
    "                    df[col] = df[col].astype(np.float64)    \r\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\r\n",
    "    if verbose: \r\n",
    "        print('StartMem:{:.2f}Mb, EndMem:{:.2f}Mb ({:.1f}% reduction)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建 特征种类-嵌入维度 对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# 特征列名称\r\n",
    "feat_list = ['android_id', 'media_id', 'package', 'apptype', 'version', \r\n",
    "             'fea_hash', 'fea1_hash', 'cus_type', 'location', 'carrier', \r\n",
    "             'dev_height', 'dev_width', 'dev_ppi', 'lan', 'ntt', 'osv', 'os']  \r\n",
    "# 各特征对应种类数\r\n",
    "cardi = [467958, 292, 2102, 89, 23, 509473, 6147, 58, 332, 5, 864, 382, 105, 25, 8, 165, 2]\r\n",
    "if USE_TIME:\r\n",
    "    feat_list.extend([\"day\", \"weekday\", \"hour\", \"minute\", \"second\"])\r\n",
    "    cardi.extend([8, 7, 24, 60, 60])\r\n",
    "# 特征列:种类数 对应字典\r\n",
    "cardi_dict = dict(zip(feat_list, cardi))    \r\n",
    "# 嵌入维度列表(嵌入维度 = 6 * (原始种类数^0.25))\r\n",
    "emb_dim_list = [6 * np.int(np.power(c, 0.25)) for c in cardi]#[EMB_DIM for c in cardi]\r\n",
    "# 构建 原始维度:嵌入维度 对应列表\r\n",
    "emb_dim_dict = list(zip(cardi, emb_dim_list))  # 每个特征域使用自己的嵌入维度\r\n",
    "emb_samedim_dict = list(zip(cardi, [EMB_DIM] * len(cardi)))  # 每个特征域是哟明相同的嵌入维度，便于基于vector-wise的特征交互\r\n",
    "# 计算嵌入总维度\r\n",
    "EMB_OUT_SIZE = sum(emb_dim_list)         \r\n",
    "EMB_SAMEOUT_SIZE = EMB_DIM * len(cardi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartMem:95.37Mb, EndMem:22.89Mb (76.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "if USE_TIME: \r\n",
    "    train = pd.read_csv(DATA_PATH + \"lbe_raw_train_with_time.csv\")\r\n",
    "else:\r\n",
    "    train = pd.read_csv(DATA_PATH + \"lbe_raw_train.csv\")\r\n",
    "train = reduce_mem_usage(train, use_uint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 分割训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTrain(df, use_all):\r\n",
    "    \"\"\"\r\n",
    "    分割训练集和验证集\r\n",
    "    :param df:全量训练数据\r\n",
    "    :param use_all:是否使用全量训练数据\r\n",
    "    :return:训练集特征, 验证集特征, 训练集标签, 验证集标签\r\n",
    "    \"\"\"\r\n",
    "    if use_all:\r\n",
    "        X_train, X_valid, y_train, y_valid = train[feat_list].values, None, train[\"label\"].values, None\r\n",
    "    else:\r\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(train[feat_list].values, \r\n",
    "                                                      train[\"label\"].values, test_size=0.1, random_state=2021)  # 0.1\r\n",
    "    \r\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = getTrain(train, USE_ALL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all, _, y_all, _ = getTrain(train, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 建立dataset类用于加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data_x, data_y):\r\n",
    "        super(MyDataset, self).__init__()\r\n",
    "        self.data_x = data_x\r\n",
    "        self.data_y = data_y\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return self.data_y.shape[0]\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.data_x[idx].astype(np.int), self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\r\n",
    "if not USE_ALL_DATA:\r\n",
    "    valid_dataset = MyDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_dataset = MyDataset(X_all, y_all)\r\n",
    "all_loader = DataLoader(all_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "if not USE_ALL_DATA:\r\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MaskNet(参考[论文链接](https://arxiv.org/pdf/2102.07619v1.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IGM(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    实例指导的Mask模块。分为两层fc，第一层将数据扩大至中间维度，第二层将中间维度投影回原始维度\r\n",
    "    :param reduce_r: 中间层缩减比例(原始维度 / 中间层维度)\r\n",
    "    :param input_size: 原始输入维度\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, reduce_r, input_size):\r\n",
    "        super(IGM, self).__init__()\r\n",
    "        \r\n",
    "        mid_size = input_size * reduce_r\r\n",
    "        self.agg_layer = nn.Linear(input_size, mid_size)  # 先扩大至中间维度\r\n",
    "        self.proj_layer = nn.Linear(mid_size, input_size)  # 再降维至原始维度\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.agg_layer(x)\r\n",
    "        x = self.proj_layer(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MaskBlock(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    MaskBlock，MaskNet的组成部分。每个MaskBlock由一个IGM和一个隐层组成。\r\n",
    "    :param emb_out_size: 嵌入层输出总维度\r\n",
    "    :param igm_reduce_r: IGM的缩减率\r\n",
    "    :param hid_size: 隐层维度，默认为-1，此时隐层维度设置为和嵌入层输出维度相同\r\n",
    "    :param mode:MaskBlock的类型，可选参数为'emb'和'mb'\r\n",
    "        \"emb\":MaskBlock on feature embedding\r\n",
    "        \"mb\":MaskBlock on MaskBlock\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, emb_out_size, igm_reduce_r, hid_size=-1, mode=\"emb\"):\r\n",
    "        \r\n",
    "        super(MaskBlock, self).__init__()\r\n",
    "        self.mode = mode  # 用于区分两种不同结构的MaskBlock\r\n",
    "        \r\n",
    "        # 三个关键部分：LN, IGM, FFH\r\n",
    "        if mode == \"emb\":\r\n",
    "            self.LN_EMB = nn.LayerNorm(emb_out_size)\r\n",
    "            \r\n",
    "        self.IGM = IGM(reduce_r=igm_reduce_r, input_size=emb_out_size)\r\n",
    "        \r\n",
    "        if hid_size == -1:\r\n",
    "            hid_size = emb_out_size\r\n",
    "        self.hid_layer = nn.Linear(emb_out_size, hid_size)\r\n",
    "        self.LN_HID = nn.LayerNorm(hid_size)\r\n",
    "        \r\n",
    "    def forward(self, x_left, x_right):\r\n",
    "        if self.mode == \"emb\":\r\n",
    "            ln_emb_out = self.LN_EMB(x_left)\r\n",
    "        else:\r\n",
    "            ln_emb_out = x_left\r\n",
    "            \r\n",
    "        igm_out = self.IGM(x_right)\r\n",
    "        out = F.relu(self.LN_HID(self.hid_layer(ln_emb_out * igm_out)))\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EmbLayer(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    嵌入层\r\n",
    "    :param emb_dict：list, [(cardi：emb_dim), ...]\r\n",
    "    :param concat_axis: \r\n",
    "        -1 -> 横向(形成一个向量)    \r\n",
    "        0 -> 纵向(形成一个矩阵)   \r\n",
    "        None -> 不拼接，直接返回嵌入list\r\n",
    "    return: 各个域嵌入之后的结果\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, emb_dict, concat_axis=-1):\r\n",
    "        super(EmbLayer, self).__init__()\r\n",
    "        self.field_num = len(emb_dict)\r\n",
    "        self.concat_axis = concat_axis\r\n",
    "        self.emb_list, self.emb_out_size = self.createEmb_list(emb_dict)\r\n",
    "        \r\n",
    "    def createEmb_list(self, emb_dict):\r\n",
    "        emb_list = nn.LayerList()\r\n",
    "        emb_out_size = 0\r\n",
    "        for cardi_num, emb_dim in emb_dict:\r\n",
    "            emb_list.append(nn.Embedding(cardi_num, emb_dim))\r\n",
    "            emb_out_size += emb_dim\r\n",
    "        \r\n",
    "        return emb_list, emb_out_size\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        if self.concat_axis is None:\r\n",
    "            return [self.emb_list[i](x[:, i]) for i in range(len(self.emb_list))]\r\n",
    "            \r\n",
    "        y = paddle.concat([self.emb_list[i](x[:, i]) for i in range(len(self.emb_list))], axis=-1)\r\n",
    "        if self.concat_axis != -1:\r\n",
    "            y = paddle.reshape(y, shape=[x.shape[0], self.field_num, -1])\r\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    MLP层\r\n",
    "    :param mlp_size_list: list, MLP各层尺寸\r\n",
    "    :param bn: bool, 是否使用batch normalization\r\n",
    "    :param drop_out: 0-1/None, 丢弃比例, 默认0.5\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, mlp_size_list, bn=False, drop_out=0.5):\r\n",
    "        super(MLP, self).__init__()\r\n",
    "        self.bn = bn\r\n",
    "        self.drop_out = drop_out\r\n",
    "        self.fc_layers = self.createFCLayers(mlp_size_list)\r\n",
    "        \r\n",
    "    def createFCLayers(self, mlp_size_list):\r\n",
    "        fc_layers = nn.LayerList()\r\n",
    "        if self.bn:\r\n",
    "            self.bn_layers = nn.LayerList()\r\n",
    "        if isinstance(self.drop_out, float):\r\n",
    "            self.drop_out_layers = nn.LayerList()\r\n",
    "\r\n",
    "        for idx in range(len(mlp_size_list)-1):\r\n",
    "            fc_layers.append(nn.Linear(mlp_size_list[idx], mlp_size_list[idx+1]))\r\n",
    "            if self.bn:\r\n",
    "                self.bn_layers.append(nn.BatchNorm1D(mlp_size_list[idx+1]))\r\n",
    "            \r\n",
    "            if isinstance(self.drop_out, float):\r\n",
    "                self.drop_out_layers.append(nn.Dropout(p=self.drop_out))\r\n",
    "\r\n",
    "        return fc_layers\r\n",
    "        \r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for idx, fc in enumerate(self.fc_layers):\r\n",
    "            x = fc(x)\r\n",
    "            if self.bn:\r\n",
    "                x = self.bn_layers[idx](x)\r\n",
    "            if isinstance(self.drop_out, float):\r\n",
    "                x = self.drop_out_layers[idx](x)\r\n",
    "            x = F.relu(x)\r\n",
    "        \r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ParaMaskNet(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    并行结构MaskNet\r\n",
    "    :param emb_dict: list, [(原始cardi数量：嵌入维度), ...]\r\n",
    "    :param igm_reduce_r_list: 每个MB对应一个IGM, 因此其长度即为MB的数量，每个值为对应的IGM中缩减率\r\n",
    "    :param mb_hid_list: 每个MB对应一个hidden层，每个值为对应的隐层数量，如果不单独设置可以全部赋值为-1，此时输出和输入维度相同\r\n",
    "    :param mlp_size_list: DNN隐层维度列表\r\n",
    "    :param emb_out_size: 嵌入层输出总维度\r\n",
    "    :param use_self_emb: 是否使用自己的嵌入层\r\n",
    "    :param use_last_fc: 是否使用最后一层全连接\r\n",
    "    :return: 并行结构的MaskNet实例\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, emb_dict, igm_reduce_r_list, mb_hid_list, mlp_size_list, emb_out_size=EMB_OUT_SIZE, use_self_emb=True, use_last_fc=True):\r\n",
    "        super(ParaMaskNet, self).__init__()\r\n",
    "        \r\n",
    "        self.emb_out_size = emb_out_size\r\n",
    "        self.use_last_fc = use_last_fc\r\n",
    "\r\n",
    "        self.use_self_emb = use_self_emb\r\n",
    "        if use_self_emb:\r\n",
    "            self.emb_layer = EmbLayer(emb_dict, concat_axis=None)\r\n",
    "        \r\n",
    "        self.mb_list, self.mb_out_size = self.createMBlist(igm_reduce_r_list, mb_hid_list)\r\n",
    "        \r\n",
    "        mlp_size_list.insert(0, self.mb_out_size)  # mb层的输出之后即为mlp层\r\n",
    "        self.mlp = MLP(mlp_size_list, bn=True, drop_out=0.5)  \r\n",
    "        \r\n",
    "        self.last_input_dim = mlp_size_list[-1]  # 最后一层fc的输入维度\r\n",
    "        if self.use_last_fc:\r\n",
    "            self.fc = nn.Linear(self.last_input_dim, 1)\r\n",
    "        \r\n",
    "    def createMBlist(self, igm_reduce_r_list, mb_hid_list):\r\n",
    "        \"\"\"\r\n",
    "        构建MaskBlock模块\r\n",
    "        :param igm_reduce_r_list: 每个MB对应的缩减率列表\r\n",
    "        :param mb_hid_list: 每个MB对应的隐层维度列表\r\n",
    "        :return: 创建好的包含若干MB的LayerList\r\n",
    "        \"\"\"\r\n",
    "        mb_list = nn.LayerList()\r\n",
    "        mb_out_size = 0  # 统计所有MB模块的总输出维度\r\n",
    "        \r\n",
    "        for i in range(len(igm_reduce_r_list)):\r\n",
    "            mb_list.append(MaskBlock(self.emb_out_size, igm_reduce_r_list[i], mb_hid_list[i]))  # \r\n",
    "            mb_out_size += self.emb_out_size if mb_hid_list[i] == -1 else mb_hid_list[i]\r\n",
    "            \r\n",
    "        return mb_list, mb_out_size\r\n",
    "        \r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        # 如果使用自己的嵌入层，则将当前数据跑一边自己的嵌入层\r\n",
    "        if self.use_self_emb:  \r\n",
    "            x = self.emb_layer(x)\r\n",
    "        # 此时x默认是未经concat的emb_list，这里进行横向拼接，形成一个长的嵌入向量\r\n",
    "        x = paddle.concat(x, axis=-1)  \r\n",
    "        # 依次经过MB模块并对结果进行拼接，依旧形成一个长的嵌入向量\r\n",
    "        x = paddle.concat([self.mb_list[i](x, x) for i in range(len(self.mb_list))], axis=-1)  \r\n",
    "        # 将MB层的输出接入MLP层\r\n",
    "        x = self.mlp(x)  \r\n",
    "        # 如果使用自身的最后一层则直接出最终计算结果，否则返回最后一层隐层的数据\r\n",
    "        if self.use_last_fc:\r\n",
    "            x = self.fc(x)\r\n",
    "            return F.sigmoid(x)\r\n",
    "            \r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModifyParaMaskNet(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    对并行MaskNet结构进行改造，原来的模块都是利用基于bit-wise的交互方式，尝试加入vector-wise的交互方式\r\n",
    "    emb_dict: 原始cardi数量：嵌入维度\r\n",
    "    igm_reduce_r_list: 每个MB对应一个IGM, 因此其长度即为MB的数量，每个值为对应的IGM中缩减率\r\n",
    "    mb_hid_list: 每个MB对应一个hidden层，每个值为对应的隐层数量，如果不单独设置可以全部赋值为-1\r\n",
    "    mlp_size_list: DNN隐层维度列表\r\n",
    "        \r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, emb_dict, igm_reduce_r_list, mb_hid_list, cin_hk_list, mlp_size_list, emb_out_size=EMB_OUT_SIZE, use_self_emb=True, use_last_fc=True):\r\n",
    "        super(ModifyParaMaskNet, self).__init__()\r\n",
    "        \r\n",
    "        self.emb_out_size = emb_out_size\r\n",
    "        self.use_last_fc = use_last_fc\r\n",
    "        \r\n",
    "        self.use_self_emb = use_self_emb\r\n",
    "        if use_self_emb:\r\n",
    "            self.emb_layer = EmbLayer(emb_dict, concat_axis=None)\r\n",
    "        \r\n",
    "        self.field_num = len(emb_dict)\r\n",
    "        self.emb_dim = emb_dict[0][1]\r\n",
    "\r\n",
    "        self.mb_list, self.mb_out_size = self.createMBlist(igm_reduce_r_list, mb_hid_list, self.emb_out_size)\r\n",
    "        \r\n",
    "        self.cin = CIN(batch_size=BATCH_SIZE, h_0=self.field_num, emb_dim=self.emb_dim, hk_list=cin_hk_list)\r\n",
    "        \r\n",
    "        mlp_size_list.insert(0, self.mb_out_size)  \r\n",
    "        if len(mlp_size_list) > 1:\r\n",
    "            self.mlp = MLP(mlp_size_list, bn=True, drop_out=True)\r\n",
    "        else:\r\n",
    "            self.mlp = None\r\n",
    "\r\n",
    "        self.last_input_dim = mlp_size_list[-1] + self.cin.cin_out_size + self.emb_out_size\r\n",
    "        if self.use_last_fc:\r\n",
    "            self.fc = nn.Linear(self.last_input_dim, 1)\r\n",
    "        \r\n",
    "    def createMBlist(self, igm_reduce_r_list, mb_hid_list, input_size):\r\n",
    "        mb_list = nn.LayerList()\r\n",
    "        mb_out_size = 0\r\n",
    "        \r\n",
    "        for i in range(len(igm_reduce_r_list)):\r\n",
    "            mb_list.append(MaskBlock(input_size, igm_reduce_r_list[i], mb_hid_list[i]))  # \r\n",
    "            mb_out_size += input_size if mb_hid_list[i] == -1 else mb_hid_list[i]\r\n",
    "            \r\n",
    "        return mb_list, mb_out_size\r\n",
    "        \r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        if self.use_self_emb:\r\n",
    "            x = self.emb_layer(x)\r\n",
    "        # 此时x默认是未经concat的emb_list\r\n",
    "        x = paddle.concat(x, axis=-1)\r\n",
    "        \r\n",
    "        cin_in = paddle.reshape(x, shape=[x.shape[0], self.field_num, self.emb_dim])  # batch_size * field_num * emb_dim\r\n",
    "        cin_out = self.cin(cin_in)\r\n",
    "        pmb_out = paddle.concat([self.mb_list[i](x, x) for i in range(len(self.mb_list))], axis=-1)\r\n",
    "              \r\n",
    "        # x = paddle.concat([x_pm, cin_out], axis=-1)\r\n",
    "        if self.mlp is not None:\r\n",
    "            mlp_out = self.mlp(pmb_out)\r\n",
    "        else:\r\n",
    "            mlp_out = pmb_out\r\n",
    "            \r\n",
    "        x = paddle.concat([x, cin_out, mlp_out], axis=-1)\r\n",
    "        if self.use_last_fc:\r\n",
    "            x = self.fc(x)\r\n",
    "            return F.sigmoid(x)\r\n",
    "            \r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_OUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SerMaskNet(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    序列MaskNet，类似于RNN的结构\r\n",
    "    :param emb_dict: [(原始种类数, 嵌入维度),...]\r\n",
    "    :param igm_reduce_r_list: list, 每个MB对应的缩减率\r\n",
    "    :param mb_hid_list: list, 每个MB对应的隐层维度\r\n",
    "    :param mlp_size_list: list, MLP各层尺寸\r\n",
    "    :param emb_out_size: 嵌入层输出总维度\r\n",
    "    :param use_self_emb: 是否使用自己的嵌入层\r\n",
    "    :param use_last_fc: 是否使用最后一层fc\r\n",
    "    :return: \r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, emb_dict, igm_reduce_r_list, mb_hid_list, mlp_size_list, emb_out_size=EMB_OUT_SIZE, use_self_emb=True, use_last_fc=True):\r\n",
    "        super(SerMaskNet, self).__init__()\r\n",
    "        \r\n",
    "        self.emb_out_size = emb_out_size\r\n",
    "        self.use_last_fc = use_last_fc\r\n",
    "\r\n",
    "        self.use_self_emb = use_self_emb\r\n",
    "        if use_self_emb:\r\n",
    "            self.emb_layer = EmbLayer(emb_dict, concat_axis=None)\r\n",
    "\r\n",
    "        self.mb_list, self.mb_out_size = self.createMBlist(igm_reduce_r_list, mb_hid_list)\r\n",
    "        \r\n",
    "        mlp_size_list.insert(0, self.mb_out_size)  # mb层的输出\r\n",
    "        if len(mlp_size_list) > 1:\r\n",
    "            self.mlp = MLP(mlp_size_list, bn=True, drop_out=0.5)\r\n",
    "        else:\r\n",
    "            self.mlp = None\r\n",
    "\r\n",
    "        # 记录最后一层输入维度\r\n",
    "        self.last_input_dim = mlp_size_list[-1]\r\n",
    "        if self.use_last_fc:\r\n",
    "            self.fc = nn.Linear(self.last_input_dim, 1)\r\n",
    "    \r\n",
    "    def createMBlist(self, igm_reduce_r_list, mb_hid_list):\r\n",
    "        mb_list = nn.LayerList()\r\n",
    "        \r\n",
    "        for i in range(len(igm_reduce_r_list)):\r\n",
    "            if i == 0:  # 第一个MaskBlock是MaskBlock on Feature Embedding\r\n",
    "                mb_list.append(MaskBlock(self.emb_out_size, igm_reduce_r_list[i], mb_hid_list[i], mode=\"emb\"))\r\n",
    "            else:  # 其余MaskBlock都是MaskBlock on MaskBlock\r\n",
    "                mb_list.append(MaskBlock(self.emb_out_size, igm_reduce_r_list[i], mb_hid_list[i], mode=\"mb\"))  # \r\n",
    "            \r\n",
    "        return mb_list, mb_hid_list[-1]\r\n",
    "\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        if self.use_self_emb:\r\n",
    "            x = self.emb_layer(x)\r\n",
    "        # 此时x是尚未concat的emb list\r\n",
    "        x = paddle.concat(x, axis=-1)\r\n",
    "\r\n",
    "        x_left = x  # 左边部分是上一个MB的输出\r\n",
    "        x_right = x  # 右边部分是原始emb层的输出\r\n",
    "        for i in range(len(self.mb_list)):\r\n",
    "            x_left = self.mb_list[i](x_left, x_right)\r\n",
    "        \r\n",
    "        if self.mlp is not None:\r\n",
    "            x = self.mlp(x_left)\r\n",
    "        else:\r\n",
    "            x = x_left\r\n",
    "        \r\n",
    "        if self.use_last_fc:\r\n",
    "            x = self.fc(x)\r\n",
    "            return F.sigmoid(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用共同的embedding层，每个模型都可以修改\r\n",
    "# 取出每个模型最后的隐层参数，然后concat之后进行fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EnsembleModel(paddle.nn.Layer):\r\n",
    "    def __init__(self, emb_dim_dict=emb_dim_dict):\r\n",
    "        super(EnsembleModel, self).__init__()\r\n",
    "        self.use_self_emb = False  # 是否使用各自自己的emb层\r\n",
    "        if not self.use_self_emb:\r\n",
    "            self.emb_layer = EmbLayer(emb_dim_dict, concat_axis=None)\r\n",
    "\r\n",
    "        self.model_list = nn.LayerList([\r\n",
    "                            ParaMaskNet(emb_dict=emb_dim_dict, igm_reduce_r_list=[1, 2, 3], mb_hid_list=[512 * i for i in range(1, 4)], \r\n",
    "                                        mlp_size_list=[512, 128], emb_out_size=EMB_OUT_SIZE, use_self_emb=self.use_self_emb, use_last_fc=False), \r\n",
    "                            # DeepFM(emb_dict=emb_dim_dict, mlp_size_list=mlp_size_list, emb_out_size=EMB_OUT_SIZE, use_self_emb=self.use_self_emb, use_last_fc=False),\r\n",
    "                            SerMaskNet(emb_dict=emb_dim_dict, igm_reduce_r_list=[1, 2, 3], mb_hid_list=[-1, -1, 512], \r\n",
    "                                        mlp_size_list=[128], emb_out_size=EMB_OUT_SIZE, use_self_emb=self.use_self_emb, use_last_fc=False)\r\n",
    "                          ])\r\n",
    "        # 还要获取最后一层拼接之后的维度\r\n",
    "        last_input_dim = sum([m.last_input_dim for m in self.model_list])\r\n",
    "        self.fc = nn.Linear(last_input_dim, 1)\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        if not self.use_self_emb:\r\n",
    "            x = self.emb_layer(x)\r\n",
    "        \r\n",
    "        out_list = [m(x) for m in self.model_list]\r\n",
    "        out = paddle.concat(out_list, axis=-1)\r\n",
    "        out = self.fc(out)\r\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### xDeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CIN(paddle.nn.Layer):\r\n",
    "    def __init__(self, batch_size, h_0, emb_dim, hk_list=[100, 100, 50]):\r\n",
    "        super(CIN, self).__init__()\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.h_0 = h_0\r\n",
    "        self.emb_dim = emb_dim\r\n",
    "        self.hk_list = hk_list\r\n",
    "        self.wk_list, self.cin_out_size = self.create_cross_ws()\r\n",
    "        \r\n",
    "    def create_cross_ws(self):\r\n",
    "        cin_out_size = 0\r\n",
    "        # 创建对应每一层的权重(也就是滤波器)\r\n",
    "        self.hk_list.insert(0, self.h_0)\r\n",
    "        wk_list = paddle.nn.ParameterList()\r\n",
    "        for k in range(1, len(self.hk_list)):\r\n",
    "            w_k = paddle.static.create_parameter(shape=[self.hk_list[k-1] * self.h_0, self.hk_list[k]], \r\n",
    "                                                dtype=\"float32\")#, \r\n",
    "                                                #default_initializer=paddle.nn.initializer.Constant(1.0))\r\n",
    "            wk_list.append(w_k)\r\n",
    "            cin_out_size += self.hk_list[k]\r\n",
    "        return wk_list, cin_out_size\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x_0 = x  # 原始输入\r\n",
    "        xk_list = [x_0]  # x_k各层的计算结果\r\n",
    "        sum_pool_list = []  # x_K各层的sum_pooling结果, 不包括第一层\r\n",
    "        # 分割原始输入\r\n",
    "        split_x_0 = paddle.split(x_0, self.emb_dim, axis=-1)\r\n",
    "        # 计算x_k\r\n",
    "        for k in range(len(self.hk_list)-1):\r\n",
    "            # 取出x(k-1)\r\n",
    "            x_km1 = xk_list[-1]\r\n",
    "            # 分割x(k-1)\r\n",
    "            split_x_km1 = paddle.split(x_km1, self.emb_dim, -1)\r\n",
    "            # 计算x(k-1) 与 x(0)的外积 -> zk\r\n",
    "            z_k = paddle.concat([paddle.matmul(split_x_km1[i], split_x_0[i], transpose_y=True).unsqueeze(0) \r\n",
    "                                                for i in range(self.emb_dim)], axis=0)  # emb_dim * batch_size * h_km1 * h_0\r\n",
    "            z_k = paddle.transpose(z_k, perm=[1, 0, 2, 3])  # batch_size * emb_dim * h_km1 * h_0\r\n",
    "            # 将zk最后两维拉成一维\r\n",
    "            z_k = paddle.reshape(z_k, shape=[z_k.shape[0], self.emb_dim, -1])  # batch_size * emb_dim * (h_km1 * h_0)\r\n",
    "            # 计算x_k = w * z_k\r\n",
    "            x_k = paddle.matmul(z_k, self.wk_list[k])  # batch_size * emb_dim * h_k\r\n",
    "            # 转置为正常形状\r\n",
    "            x_k = paddle.transpose(x_k, perm=[0, 2, 1])  # batch_size * h_k * emb_dim\r\n",
    "            # 存储x_k\r\n",
    "            xk_list.append(x_k)\r\n",
    "            # 计算sum_pooling\r\n",
    "            sum_pool_list.append(paddle.sum(x_k, axis=-1))\r\n",
    "            #print(x_k.shape)\r\n",
    "        \r\n",
    "        # concat sum_pooling的结果\r\n",
    "        return paddle.concat(sum_pool_list, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class xDeepFM(nn.Layer):\r\n",
    "    def __init__(self, batch_size, emb_dict, emb_dim, cin_hk_list, mlp_size_list):\r\n",
    "        super(xDeepFM, self).__init__()\r\n",
    "        self.emb_layer = EmbLayer(emb_dict)\r\n",
    "        self.field_num = len(emb_dict)\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.emb_dim = emb_dim\r\n",
    "        self.cin = CIN(batch_size=batch_size, h_0=self.field_num, emb_dim=emb_dim, hk_list=cin_hk_list)\r\n",
    "\r\n",
    "        # 嵌入总维度 = 域数量 * 单个嵌入维度\r\n",
    "        mlp_size_list.insert(0, self.emb_layer.emb_out_size)  # 域数量 * 嵌入维度\r\n",
    "        self.mlp = MLP(mlp_size_list, bn=True, drop_out=True)\r\n",
    "        self.fc = nn.Linear(self.emb_layer.emb_out_size + self.cin.cin_out_size + mlp_size_list[-1], 1)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb_layer(x)  # batch_size * (field_num * emb_dim)\r\n",
    "        # 将原始一维嵌入转换为二维的形式\r\n",
    "        cin_in = paddle.reshape(x, shape=[x.shape[0], self.field_num, self.emb_dim])  # batch_size * field_num * emb_dim\r\n",
    "        # 计算cin的输出\r\n",
    "        cin_out = self.cin(cin_in)\r\n",
    "        # 计算mlp的输出\r\n",
    "        mlp_out = self.mlp(x)\r\n",
    "        # 拼接 原始嵌入 CIN MLP\r\n",
    "        x = paddle.concat([x, cin_out, mlp_out], axis=-1)\r\n",
    "        x = F.sigmoid(self.fc(x))\r\n",
    "        return x\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "emb_dict =  list(zip(cardi, [6 * np.int(np.power(c, 0.25)) for c in cardi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PlainDNN(nn.Layer):\r\n",
    "    def __init__(self, emb_dict, emb_dim, mlp_size_list):\r\n",
    "        super(PlainDNN, self).__init__()\r\n",
    "        self.emb_layer = EmbLayer(emb_dict, concat_axis=None)\r\n",
    "        mlp_size_list.insert(0, self.emb_layer.emb_out_size)\r\n",
    "        self.mlp = MLP(mlp_size_list, bn=True, drop_out=True)\r\n",
    "        self.fc = nn.Linear(mlp_size_list[-1], 1)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb_layer(x)\r\n",
    "        x = paddle.concat(x, axis=-1)\r\n",
    "        x = self.mlp(x)\r\n",
    "        x = F.sigmoid(self.fc(x))\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepFM(nn.Layer):\r\n",
    "    def __init__(self, emb_dict, mlp_size_list, emb_out_size=EMB_OUT_SIZE, use_self_emb=True, use_last_fc=True):\r\n",
    "        super(DeepFM, self).__init__()\r\n",
    "        \r\n",
    "        self.use_self_emb = use_self_emb\r\n",
    "        self.use_last_fc = use_last_fc\r\n",
    "        # embedding\r\n",
    "        feat_num = len(emb_dict)\r\n",
    "        if self.use_self_emb:\r\n",
    "            self.emb_layer = EmbLayer(emb_dict, concat_axis=None)\r\n",
    "\r\n",
    "        self.cross_w_list = nn.ParameterList([paddle.static.create_parameter(shape=[1, 32], dtype=\"float32\")] * feat_num)  # 为每个特征域生成一个对应的可学习参数，用于交叉\r\n",
    "        self.emb_out_size = emb_out_size\r\n",
    "\r\n",
    "        self.sum_w_list = nn.ParameterList([paddle.static.create_parameter(shape=[1, 1], dtype=\"float32\")] * \r\n",
    "                                          ((feat_num * (feat_num-1))//2 + feat_num))\r\n",
    "        # mlp\r\n",
    "        mlp_size_list.insert(0, self.emb_out_size)\r\n",
    "        self.mlp = MLP(mlp_size_list, bn=True, drop_out=True)\r\n",
    "        \r\n",
    "        # print(self.emb_out_size)\r\n",
    "        self.last_input_dim = self.emb_out_size//feat_num + mlp_size_list[-1]\r\n",
    "        # 拼接fm和mlp\r\n",
    "        if self.use_last_fc:\r\n",
    "            self.fc = nn.Linear(self.last_input_dim, 1)\r\n",
    "        \r\n",
    "    def crossFeats(self, emb_out_list):\r\n",
    "        cross_out_list = []\r\n",
    "        # 对应位置权重相乘\r\n",
    "        for i in range(len(emb_out_list)):\r\n",
    "            for j in range(i+1, len(emb_out_list)):\r\n",
    "                cross_out_list.append(self.cross_w_list[i].dot(self.cross_w_list[j]) * emb_out_list[i] * emb_out_list[j])\r\n",
    "        return cross_out_list\r\n",
    "    \r\n",
    "    def sumFeats(self, cross_list):\r\n",
    "        out = paddle.zeros_like(cross_list[0])\r\n",
    "        for i in range(len(cross_list)):\r\n",
    "            out += self.sum_w_list[i] * cross_list[i]\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # 对所有特征进行依次嵌入\r\n",
    "        if self.use_self_emb:\r\n",
    "            emb_out_list = self.emb_layer(x)\r\n",
    "        else:  # 如果不使用自己的emb层，公用层输出默认就是emb之后的List\r\n",
    "            emb_out_list = x\r\n",
    "\r\n",
    "        # 计算二阶特征交叉，返回交叉后的List\r\n",
    "        cross_out_list = self.crossFeats(emb_out_list)\r\n",
    "        # 将所有交叉过的2阶特征和原始1阶特征concat\r\n",
    "        #shallow_out = paddle.concat(emb_out_list + cross_out_list, axis=1)\r\n",
    "        # 将一阶特征(原始emb输出)和二阶交叉list拼接之后求和\r\n",
    "        shallow_out = self.sumFeats(emb_out_list + cross_out_list)  # bz * (n + (n-1)*n/2) * emb_dim -> bz * emb_dim\r\n",
    "        \r\n",
    "        # DNN输出\r\n",
    "        deep_out = self.mlp(paddle.concat(emb_out_list, axis=1))\r\n",
    "        \r\n",
    "        out = paddle.concat([shallow_out, deep_out], axis=-1)\r\n",
    "        \r\n",
    "        if self.use_last_fc:\r\n",
    "            out = self.fc(out)\r\n",
    "            return F.sigmoid(out)\r\n",
    "        # 如果不使用最后一层fc，则直接把当前模型训练的最后一个隐层输出，用于模型融合\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DeepFFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepFFM(nn.Layer):\r\n",
    "    def __init__(self, cardi_dict, emb_dim, dnn_list):\r\n",
    "        super(DeepFFM, self).__init__()\r\n",
    "        \r\n",
    "        self.cardi_dict = cardi_dict\r\n",
    "        self.emb_layers = nn.LayerList()\r\n",
    "        self.cross_w_list = nn.ParameterList()\r\n",
    "        \r\n",
    "        self.emb_dim_sum = self.createEmbeddingLayers(emb_dim)\r\n",
    "        \r\n",
    "        dnn_list.insert(0, self.emb_dim_sum)\r\n",
    "        self.mlp = MLP(dnn_list, bn=True, drop_out=True)\r\n",
    "        feat_num = len(cardi_dict)\r\n",
    "        self.sum_w_list = nn.ParameterList([paddle.static.create_parameter(shape=[1, 1], dtype=\"float32\")] * \r\n",
    "                                          (np.int(np.power(feat_num-1, 2)) + feat_num))\r\n",
    "        #last_dim = np.int(np.power(feat_num-1, 2)) * emb_dim + feat_num * emb_dim + dnn_list[-1]\r\n",
    "        last_dim = emb_dim + dnn_list[-1]\r\n",
    "        self.fc = nn.Linear(last_dim, 1)\r\n",
    "        \r\n",
    "    def createEmbeddingLayers(self, emb_dim):\r\n",
    "        emb_dim_sum = 0\r\n",
    "        for feat in feat_list:\r\n",
    "            cardi = self.cardi_dict[feat]\r\n",
    "            self.emb_layers.append(nn.Embedding(cardi, emb_dim))\r\n",
    "            # 每个特征域对应一个权重矩阵: emb_dim * m\r\n",
    "            w_dim = 16\r\n",
    "            self.cross_w_list.append(paddle.static.create_parameter(shape=[emb_dim, w_dim], dtype=\"float32\"))  \r\n",
    "            emb_dim_sum += emb_dim\r\n",
    "        \r\n",
    "        return emb_dim_sum\r\n",
    "    \r\n",
    "    def crossFeats(self, emb_out_list):\r\n",
    "        cross_out_list = []\r\n",
    "        # 对应位置权重相乘\r\n",
    "        for i in range(len(emb_out_list)):\r\n",
    "            for j in range(len(emb_out_list)):\r\n",
    "                if i == j: \r\n",
    "                    break\r\n",
    "                hadma_prod = emb_out_list[i] * emb_out_list[j]\r\n",
    "                w_prod = paddle.matmul(self.cross_w_list[i], self.cross_w_list[j], transpose_y=True)\r\n",
    "                w_emb_prod = paddle.matmul(w_prod, hadma_prod, transpose_y=True)\r\n",
    "                cross_out_list.append(paddle.transpose(w_emb_prod, perm=[1, 0]))\r\n",
    "        return cross_out_list\r\n",
    "    \r\n",
    "    def sumFeats(self, cross_list):\r\n",
    "        out = paddle.zeros_like(cross_list[0])\r\n",
    "        for i in range(len(cross_list)):\r\n",
    "            out += self.sum_w_list[i] * cross_list[i]\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # 对所有特征进行依次嵌入\r\n",
    "        emb_out_list = [self.emb_layers[i](x[:, i]) for i in range(len(self.emb_layers))]\r\n",
    "        # 对所有特征进行交叉\r\n",
    "        cross_out_list = self.crossFeats(emb_out_list)\r\n",
    "        # 将所有交叉过的2阶特征和原始1阶特征concat\r\n",
    "        #shallow_out = paddle.concat(emb_out_list + cross_out_list, axis=1)\r\n",
    "        shallow_out = self.sumFeats(emb_out_list + cross_out_list)  # bz * 64\r\n",
    "        \r\n",
    "        # DNN输出\r\n",
    "        deep_out = self.mlp(paddle.concat(emb_out_list, axis=1))\r\n",
    "        \r\n",
    "        out = paddle.concat([shallow_out, deep_out], axis=-1)\r\n",
    "        \r\n",
    "        out = self.fc(out)\r\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model_dffm = DeepFFM(cardi_dict=cardi_dict, emb_dim=64, dnn_list=[512, 256, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_size_list = [512, 256, 64]#[400, 400, 400]#[512, 256, 64] 倒金字塔结构效果要稍微比全相同结构效果好点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_OUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dfm = DeepFM(emb_dict=emb_dim_dict, mlp_size_list=[512, 256, 64], emb_out_size=EMB_OUT_SIZE, use_self_emb=True, use_last_fc=True)\r\n",
    "#model_dfm = DeepFM(cardi_dict=cardi_dict, emb_dim=64, dnn_list=[512, 256, 64])\r\n",
    "#optimizer = optim.Adam(parameters=model_dfm.parameters(), learning_rate=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_xdfm = xDeepFM(batch_size=BATCH_SIZE, \r\n",
    "                    emb_dict=emb_dim_dict, \r\n",
    "                    emb_dim=CIN_EMB_DIM, \r\n",
    "                    cin_hk_list=CIN_HK_LIST, \r\n",
    "                    mlp_size_list=[512, 256, 64])#[256, 256, 32])#[512, 256, 64])#[128, 256, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "igm_reduce_r_list = [i for i in range(1, MB_NUMS+1)]\r\n",
    "mlp_size_list=[512, 256, 64]#[400, 400, 400]#[512, 256, 64]  # [400, 400, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_OUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pmb = ParaMaskNet(emb_dict=emb_dim_dict, \r\n",
    "                        igm_reduce_r_list=[1, 2, 3], \r\n",
    "                        mb_hid_list=[512, 512, 512], \r\n",
    "                        mlp_size_list=[512, 256, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_smb = SerMaskNet(emb_dict=emb_dim_dict, \r\n",
    "                        igm_reduce_r_list=igm_reduce_r_list, \r\n",
    "                        mb_hid_list=[-1] * MB_NUMS, \r\n",
    "                        mlp_size_list=mlp_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ensem = EnsembleModel(emb_dim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTrue(a, b):\r\n",
    "    res = 0\r\n",
    "    for i in range(len(a)):\r\n",
    "        if a[i] == b[i]:\r\n",
    "            res += 1\r\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validation(model, loss_fn, data_loader, mode=\"valid\"):\r\n",
    "    model.eval()\r\n",
    "    acc = 0\r\n",
    "    loss_ave = 0\r\n",
    "    for idx, (x, y) in enumerate(data_loader):\r\n",
    "        y = y.numpy()\r\n",
    "        x = paddle.to_tensor(x)\r\n",
    "        y = paddle.to_tensor(y, dtype=\"float32\")\r\n",
    "        y_pre = model(x)\r\n",
    "        y_pre = y_pre.squeeze(-1)\r\n",
    "        loss = loss_fn(y_pre, y)\r\n",
    "        loss_ave += loss.numpy()[0]\r\n",
    "        a = y_pre.numpy()\r\n",
    "        a = list(map(lambda x: 1 if x >= 0.5 else 0, a))\r\n",
    "        b = y.numpy()\r\n",
    "        b = list(map(lambda x: np.int(x), b))\r\n",
    "        acc += getTrue(a, b)\r\n",
    "    acc /= len(data_loader) * BATCH_SIZE\r\n",
    "    loss_ave /= len(data_loader) * BATCH_SIZE\r\n",
    "    print(mode + f\":loss_ave:{loss_ave}, acc:{acc}\")\r\n",
    "    model.train()\r\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainModel(model, loss_fn, optimizer, device, data_loader, start_epoch=0, epoches=10, model_name=\"DNN\", start_save_acc=0.8801, use_all_data=USE_ALL_DATA):\r\n",
    "    print(device)\r\n",
    "    paddle.set_device(device)\r\n",
    "    model.train()\r\n",
    "    loss_list = []\r\n",
    "    loss_min = 9999\r\n",
    "    acc_list = []\r\n",
    "    acc_max = start_save_acc\r\n",
    "    #for epoch in range(epoches):\r\n",
    "    for epoch in range(start_epoch, start_epoch + epoches):\r\n",
    "        epoch_ave_loss = 0\r\n",
    "        for idx, (x, y) in enumerate(data_loader):\r\n",
    "            y = y.numpy()\r\n",
    "            x = paddle.to_tensor(x)\r\n",
    "            y = paddle.to_tensor(y, dtype=\"float32\")\r\n",
    "            y_pre = model(x)\r\n",
    "            y_pre = y_pre.squeeze(-1)\r\n",
    "            loss = loss_fn(y_pre, y)\r\n",
    "            \r\n",
    "            epoch_ave_loss += loss.numpy()[0]\r\n",
    "            if idx % 500 == 0:\r\n",
    "                print(f\"epoch:{epoch}, idx:{idx}, batch_sum_loss:{loss.numpy()[0]}\")\r\n",
    "\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "        \r\n",
    "        epoch_ave_loss /= len(data_loader) * BATCH_SIZE\r\n",
    "        loss_list.append(epoch_ave_loss)\r\n",
    "        print(f\"epoch:{epoch}, ave_loss:{epoch_ave_loss}\")\r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "        # 计算训练集准确率\r\n",
    "        train_acc = validation(model, loss_fn, data_loader, mode=\"train\")\r\n",
    "        \r\n",
    "        if not use_all_data:\r\n",
    "            acc = validation(model, loss_fn, valid_loader)\r\n",
    "            acc_list.append(acc)\r\n",
    "            if acc > acc_max:\r\n",
    "                acc_max = acc\r\n",
    "                paddle.save(model.state_dict(), \"./work/\" + model_name + \"_acc_epoch\" + str(epoch) + \".pdparams\")\r\n",
    "        else:\r\n",
    "            if epoch_ave_loss < loss_min:\r\n",
    "                loss_min = epoch_ave_loss\r\n",
    "                paddle.save(model.state_dict(), \"./work/\" + model_name + \"_loss_epoch\" + str(epoch) + \".pdparams\")\r\n",
    "        print(\"-------------------------------------------------------------------\")\r\n",
    "        \r\n",
    "        if epoch_ave_loss < 0.27:\r\n",
    "            print(\"Early Stopping!\")\r\n",
    "            return loss_list, acc_list\r\n",
    "\r\n",
    "    return loss_list, acc_list\r\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction=\"sum\")\r\n",
    "device = paddle.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:3, idx:0, batch_sum_loss:44.73744583129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, idx:500, batch_sum_loss:34.55804443359375\n",
      "epoch:3, idx:1000, batch_sum_loss:31.3226375579834\n",
      "epoch:3, idx:1500, batch_sum_loss:38.73979568481445\n",
      "epoch:3, ave_loss:0.13989192146523957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss_ave:0.3035193509137144, acc:0.8828523596938775\n",
      "-------------------------------------------------------------------\n",
      "epoch:4, idx:0, batch_sum_loss:37.36035919189453\n",
      "epoch:4, idx:500, batch_sum_loss:23.54827117919922\n",
      "epoch:4, idx:1000, batch_sum_loss:22.998573303222656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e5ea07914a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_dfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dfm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_save_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.884\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-b346cd0a0da0>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, loss_fn, optimizer, device, data_loader, start_epoch, epoches, model_name, start_save_acc)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b142d00f04d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 计算二阶特征交叉，返回交叉后的List\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mcross_out_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossFeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_out_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# 将所有交叉过的2阶特征和原始1阶特征concat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#shallow_out = paddle.concat(emb_out_list + cross_out_list, axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b142d00f04d9>\u001b[0m in \u001b[0;36mcrossFeats\u001b[0;34m(self, emb_out_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_out_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_out_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mcross_out_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_w_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_w_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0memb_out_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0memb_out_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_out_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(self, other_var)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mmath_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmath_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpProtoHolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_op_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model_param = paddle.load(\"./work/dfm_acc_epoch2_8834.pdparams\")\r\n",
    "# model_dfm.set_state_dict(model_param)\r\n",
    "optimizer = optim.Adam(parameters=model_dfm.parameters(), learning_rate=0.001, weight_decay=0.01)\r\n",
    "loss_list, acc_list = trainModel(model_dfm, loss_fn, optimizer, device, train_loader, start_epoch=3, epoches=20, model_name=\"dfm\", start_save_acc=0.884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:21, idx:0, batch_sum_loss:76.312255859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21, idx:500, batch_sum_loss:70.6458969116211\n",
      "epoch:21, idx:1000, batch_sum_loss:83.3962173461914\n",
      "epoch:21, idx:1500, batch_sum_loss:77.32967376708984\n",
      "epoch:21, ave_loss:0.276159700196896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:loss_ave:0.27005983539356515, acc:0.8973719773541453\n",
      "-------------------------------------------------------------------\n",
      "epoch:22, idx:0, batch_sum_loss:79.58964538574219\n",
      "epoch:22, idx:500, batch_sum_loss:59.79456329345703\n",
      "epoch:22, idx:1000, batch_sum_loss:74.67666625976562\n",
      "epoch:22, idx:1500, batch_sum_loss:66.36986541748047\n",
      "epoch:22, ave_loss:0.2736798601793154\n",
      "train:loss_ave:0.2682259527641323, acc:0.8982056038894575\n",
      "-------------------------------------------------------------------\n",
      "epoch:23, idx:0, batch_sum_loss:77.4429931640625\n",
      "epoch:23, idx:500, batch_sum_loss:69.73820495605469\n",
      "epoch:23, idx:1000, batch_sum_loss:69.97480010986328\n",
      "epoch:23, idx:1500, batch_sum_loss:75.87858581542969\n",
      "epoch:23, ave_loss:0.27168930376457584\n",
      "train:loss_ave:0.26387052942818545, acc:0.9008864028915046\n",
      "-------------------------------------------------------------------\n",
      "epoch:24, idx:0, batch_sum_loss:84.5690689086914\n",
      "epoch:24, idx:500, batch_sum_loss:37.56404113769531\n",
      "epoch:24, idx:1000, batch_sum_loss:75.11445617675781\n",
      "epoch:24, idx:1500, batch_sum_loss:68.24108123779297\n",
      "epoch:24, ave_loss:0.25303255163415306\n",
      "train:loss_ave:0.19119039730968654, acc:0.9375159928352098\n",
      "-------------------------------------------------------------------\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "model_ensem = EnsembleModel(emb_dim_dict=emb_dim_dict)\r\n",
    "# model_param = paddle.load(\"./work/ens_pds_loss_epoch17.pdparams\")\r\n",
    "model_ensem.set_state_dict(model_param)\r\n",
    "optimizer = optim.Adam(parameters=model_ensem.parameters(), learning_rate=0.001, weight_decay=0.01)\r\n",
    "loss_list, acc_list = trainModel(model_ensem, loss_fn, optimizer, device, train_loader, start_epoch=1, epoches=20, model_name=\"ens_pds\", start_save_acc=0, use_all_data=Fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_OUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:16, idx:0, batch_sum_loss:61.516822814941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, idx:500, batch_sum_loss:51.78311538696289\n",
      "epoch:16, idx:1000, batch_sum_loss:58.714622497558594\n",
      "epoch:16, idx:1500, batch_sum_loss:73.0323486328125\n",
      "epoch:16, ave_loss:0.2697332961838671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:loss_ave:0.26406918475414976, acc:0.8992996302616609\n",
      "valid:loss_ave:0.2944936845831725, acc:0.8865393813775511\n",
      "-------------------------------------------------------------------\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "# model_pmb = ParaMaskNet(emb_dict=emb_dim_dict, \r\n",
    "#                         igm_reduce_r_list=[1, 2, 3], \r\n",
    "#                         mb_hid_list=[256, 512, 768], \r\n",
    "#                         mlp_size_list=[512])\r\n",
    "model_param = paddle.load(\"./work/pmb_acc_epoch15_8869.pdparams\")\r\n",
    "model_pmb.set_state_dict(model_param)\r\n",
    "optimizer = optim.Adam(parameters=model_pmb.parameters(), learning_rate=0.0001, weight_decay=0.01)\r\n",
    "loss_list, acc_list = trainModel(model_pmb, loss_fn, optimizer, device, train_loader, start_epoch=16, epoches=10, model_name=\"pmb\", start_save_acc=0.887, use_all_data=False)\r\n",
    "# loss_list, acc_list = trainModel(model_pmb, loss_fn, optimizer, device, valid_loader, start_epoch=1, epoches=2, model_name=\"pmb\", start_save_acc=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paddle.save(model_pmb.state_dict(), \"./work/pmb_879\" + \".pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:11, idx:0, batch_sum_loss:68.77133178710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11, idx:500, batch_sum_loss:74.66376495361328\n",
      "epoch:11, idx:1000, batch_sum_loss:68.3926010131836\n",
      "epoch:11, idx:1500, batch_sum_loss:67.22955322265625\n",
      "epoch:11, ave_loss:0.2755413932092831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:loss_ave:0.2665813494084771, acc:0.8992211489252815\n",
      "-------------------------------------------------------------------\n",
      "epoch:12, idx:0, batch_sum_loss:74.54212188720703\n",
      "epoch:12, idx:500, batch_sum_loss:62.368614196777344\n",
      "epoch:12, idx:1000, batch_sum_loss:57.9099235534668\n",
      "epoch:12, idx:1500, batch_sum_loss:76.84490966796875\n",
      "epoch:12, ave_loss:0.24715328806803438\n",
      "train:loss_ave:0.1671947797554466, acc:0.9479473196008188\n",
      "-------------------------------------------------------------------\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "# model_smb = ParaMaskNet(emb_dict=emb_dim_dict, \r\n",
    "#                         igm_reduce_r_list=[1, 2, 3], \r\n",
    "#                         mb_hid_list=[-1, -1, 512], \r\n",
    "#                         mlp_size_list=[128])\r\n",
    "model_param = paddle.load(\"./work/smb_loss_epoch10.pdparams\")\r\n",
    "model_smb.set_state_dict(model_param)\r\n",
    "optimizer = optim.Adam(parameters=model_smb.parameters(), learning_rate=0.0001, weight_decay=0.01)\r\n",
    "loss_list, acc_list = trainModel(model_smb, loss_fn, optimizer, device, train_loader, start_epoch=11, epoches=5, model_name=\"smb\", start_save_acc=0)\r\n",
    "# loss_list, acc_list = trainModel(model_pmb, loss_fn, optimizer, device, valid_loader, start_epoch=1, epoches=2, model_name=\"pmb\", start_save_acc=0.88)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:1, idx:0, batch_sum_loss:179.44302368164062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, idx:500, batch_sum_loss:83.90325927734375\n",
      "epoch:1, idx:1000, batch_sum_loss:74.12667846679688\n",
      "epoch:1, idx:1500, batch_sum_loss:90.74762725830078\n",
      "epoch:1, ave_loss:0.3068562645350707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:loss_ave:0.28122092282507605, acc:0.8957400099544938\n",
      "valid:loss_ave:0.2947523988479254, acc:0.8838687818877551\n",
      "-------------------------------------------------------------------\n",
      "epoch:2, idx:0, batch_sum_loss:58.34437942504883\n",
      "epoch:2, idx:500, batch_sum_loss:253.88067626953125\n",
      "epoch:2, idx:1000, batch_sum_loss:63.21255111694336\n",
      "epoch:2, idx:1500, batch_sum_loss:62.499237060546875\n",
      "epoch:2, ave_loss:0.27252886153596523\n",
      "train:loss_ave:0.11830356762457354, acc:0.9581622404721274\n",
      "valid:loss_ave:0.2920865372811653, acc:0.8861407844387755\n",
      "-------------------------------------------------------------------\n",
      "epoch:3, idx:0, batch_sum_loss:25.031949996948242\n",
      "epoch:3, idx:500, batch_sum_loss:3.4534337520599365\n",
      "epoch:3, idx:1000, batch_sum_loss:19.38315200805664\n",
      "epoch:3, idx:1500, batch_sum_loss:9.734344482421875\n",
      "epoch:3, ave_loss:0.04545383344894959\n",
      "train:loss_ave:0.011859934718946188, acc:0.9970736454778157\n",
      "valid:loss_ave:0.34348469273168214, acc:0.8719308035714286\n",
      "-------------------------------------------------------------------\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "model_mpmb = ModifyParaMaskNet(emb_dict=emb_samedim_dict, \n",
    "                        igm_reduce_r_list=[1, 2, 3], \n",
    "                        mb_hid_list=[256, 512, 768], \n",
    "                        cin_hk_list=[128, 128, 64],\n",
    "                        mlp_size_list=[256],\n",
    "                        emb_out_size=EMB_SAMEOUT_SIZE)\n",
    "# model_param = paddle.load(\"./work/pmb_acc_epoch15_8869.pdparams\")\n",
    "# model_mpmb.set_state_dict(model_param)\n",
    "optimizer = optim.Adam(parameters=model_mpmb.parameters(), learning_rate=0.001, weight_decay=0.01)\n",
    "loss_list, acc_list = trainModel(model_mpmb, loss_fn, optimizer, device, train_loader, start_epoch=1, epoches=10, model_name=\"mpmb\", start_save_acc=0.886, use_all_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_param = paddle.load(\"./work/ens_pds_acc_epoch11_8850.pdparams\")\r\n",
    "model_ensem.set_state_dict(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0\n",
      "epoch:33, idx:0, batch_sum_loss:40.37668228149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33, idx:500, batch_sum_loss:44.81654739379883\n",
      "epoch:33, idx:1000, batch_sum_loss:37.065853118896484\n",
      "epoch:33, idx:1500, batch_sum_loss:37.669883728027344\n",
      "epoch:33, idx:2000, batch_sum_loss:42.260440826416016\n",
      "epoch:33, idx:2500, batch_sum_loss:27.230144500732422\n",
      "epoch:33, idx:3000, batch_sum_loss:36.9178581237793\n",
      "epoch:33, idx:3500, batch_sum_loss:41.12592315673828\n",
      "epoch:33, ave_loss:0.2936027578297532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss_ave:0.2951925071837652, acc:0.8871283567774936\n",
      "-------------------------------------------------------------------\n",
      "epoch:34, idx:0, batch_sum_loss:29.279850006103516\n",
      "epoch:34, idx:500, batch_sum_loss:37.04157257080078\n",
      "epoch:34, idx:1000, batch_sum_loss:54.20576095581055\n",
      "epoch:34, idx:1500, batch_sum_loss:49.444889068603516\n",
      "epoch:34, idx:2000, batch_sum_loss:36.44204330444336\n",
      "epoch:34, idx:2500, batch_sum_loss:40.47740936279297\n",
      "epoch:34, idx:3000, batch_sum_loss:32.983436584472656\n",
      "epoch:34, idx:3500, batch_sum_loss:24.201622009277344\n",
      "epoch:34, ave_loss:0.2930525503289781\n",
      "valid_loss_ave:0.29500971646869883, acc:0.8869085677749361\n",
      "-------------------------------------------------------------------\n",
      "epoch:35, idx:0, batch_sum_loss:49.15219497680664\n",
      "epoch:35, idx:500, batch_sum_loss:38.6931266784668\n",
      "epoch:35, idx:1000, batch_sum_loss:29.52236557006836\n",
      "epoch:35, idx:1500, batch_sum_loss:37.406097412109375\n",
      "epoch:35, idx:2000, batch_sum_loss:38.271324157714844\n",
      "epoch:35, idx:2500, batch_sum_loss:33.226253509521484\n",
      "epoch:35, idx:3000, batch_sum_loss:29.009052276611328\n",
      "epoch:35, idx:3500, batch_sum_loss:43.20220947265625\n",
      "epoch:35, ave_loss:0.2927812091811254\n",
      "valid_loss_ave:0.2950535266448165, acc:0.8871283567774936\n",
      "-------------------------------------------------------------------\n",
      "epoch:36, idx:0, batch_sum_loss:31.235889434814453\n",
      "epoch:36, idx:500, batch_sum_loss:39.482852935791016\n",
      "epoch:36, idx:1000, batch_sum_loss:34.73899841308594\n",
      "epoch:36, idx:1500, batch_sum_loss:43.83572769165039\n",
      "epoch:36, idx:2000, batch_sum_loss:37.899593353271484\n",
      "epoch:36, idx:2500, batch_sum_loss:47.61854553222656\n",
      "epoch:36, idx:3000, batch_sum_loss:53.184173583984375\n",
      "epoch:36, idx:3500, batch_sum_loss:31.44452476501465\n",
      "epoch:36, ave_loss:0.2922395810196695\n",
      "valid_loss_ave:0.2949185876742653, acc:0.887028452685422\n",
      "-------------------------------------------------------------------\n",
      "epoch:37, idx:0, batch_sum_loss:46.449981689453125\n",
      "epoch:37, idx:500, batch_sum_loss:33.278648376464844\n",
      "epoch:37, idx:1000, batch_sum_loss:30.948715209960938\n",
      "epoch:37, idx:1500, batch_sum_loss:42.437904357910156\n",
      "epoch:37, idx:2000, batch_sum_loss:38.27960968017578\n",
      "epoch:37, idx:2500, batch_sum_loss:46.123146057128906\n",
      "epoch:37, idx:3000, batch_sum_loss:26.25926971435547\n",
      "epoch:37, idx:3500, batch_sum_loss:33.62371826171875\n",
      "epoch:37, ave_loss:0.29189080635214965\n",
      "valid_loss_ave:0.2950488719565179, acc:0.8872882033248082\n",
      "-------------------------------------------------------------------\n",
      "epoch:38, idx:0, batch_sum_loss:39.431663513183594\n",
      "epoch:38, idx:500, batch_sum_loss:38.05466842651367\n",
      "epoch:38, idx:1000, batch_sum_loss:33.46601486206055\n",
      "epoch:38, idx:1500, batch_sum_loss:31.85282325744629\n",
      "epoch:38, idx:2000, batch_sum_loss:36.3635139465332\n",
      "epoch:38, idx:2500, batch_sum_loss:37.97563934326172\n",
      "epoch:38, idx:3000, batch_sum_loss:35.786861419677734\n",
      "epoch:38, idx:3500, batch_sum_loss:35.594451904296875\n",
      "epoch:38, ave_loss:0.29098215539419475\n",
      "valid_loss_ave:0.2949965724249935, acc:0.8870684143222506\n",
      "-------------------------------------------------------------------\n",
      "epoch:39, idx:0, batch_sum_loss:31.846111297607422\n",
      "epoch:39, idx:500, batch_sum_loss:30.909040451049805\n",
      "epoch:39, idx:1000, batch_sum_loss:27.55674171447754\n",
      "epoch:39, idx:1500, batch_sum_loss:27.85861587524414\n",
      "epoch:39, idx:2000, batch_sum_loss:36.807220458984375\n",
      "epoch:39, idx:2500, batch_sum_loss:35.17938232421875\n",
      "epoch:39, idx:3000, batch_sum_loss:44.797786712646484\n",
      "epoch:39, idx:3500, batch_sum_loss:39.03993225097656\n",
      "epoch:39, ave_loss:0.28934053091737055\n",
      "valid_loss_ave:0.29545996477231956, acc:0.8869085677749361\n",
      "-------------------------------------------------------------------\n",
      "epoch:40, idx:0, batch_sum_loss:31.173912048339844\n",
      "epoch:40, idx:500, batch_sum_loss:27.184619903564453\n",
      "epoch:40, idx:1000, batch_sum_loss:29.021032333374023\n",
      "epoch:40, idx:1500, batch_sum_loss:43.98846435546875\n",
      "epoch:40, idx:2000, batch_sum_loss:43.14198303222656\n",
      "epoch:40, idx:2500, batch_sum_loss:46.62287902832031\n",
      "epoch:40, idx:3000, batch_sum_loss:34.709800720214844\n",
      "epoch:40, idx:3500, batch_sum_loss:30.975175857543945\n",
      "epoch:40, ave_loss:0.2853881121232716\n",
      "valid_loss_ave:0.29756805365500244, acc:0.8861293158567775\n",
      "-------------------------------------------------------------------\n",
      "epoch:41, idx:0, batch_sum_loss:37.772708892822266\n",
      "epoch:41, idx:500, batch_sum_loss:39.79104995727539\n",
      "epoch:41, idx:1000, batch_sum_loss:30.27808952331543\n",
      "epoch:41, idx:1500, batch_sum_loss:23.307361602783203\n",
      "epoch:41, idx:2000, batch_sum_loss:32.910343170166016\n",
      "epoch:41, idx:2500, batch_sum_loss:34.94141387939453\n",
      "epoch:41, idx:3000, batch_sum_loss:35.26411056518555\n",
      "epoch:41, idx:3500, batch_sum_loss:38.19464874267578\n",
      "epoch:41, ave_loss:0.27947127296927304\n",
      "valid_loss_ave:0.3015995170835339, acc:0.8838115409207161\n",
      "-------------------------------------------------------------------\n",
      "epoch:42, idx:0, batch_sum_loss:38.34397888183594\n",
      "epoch:42, idx:500, batch_sum_loss:36.645408630371094\n",
      "epoch:42, idx:1000, batch_sum_loss:20.427410125732422\n",
      "epoch:42, idx:1500, batch_sum_loss:35.6138916015625\n",
      "epoch:42, idx:2000, batch_sum_loss:39.67095947265625\n",
      "epoch:42, idx:2500, batch_sum_loss:36.14746856689453\n",
      "epoch:42, idx:3000, batch_sum_loss:32.90156555175781\n",
      "epoch:42, idx:3500, batch_sum_loss:32.292423248291016\n",
      "epoch:42, ave_loss:0.27231340685836014\n",
      "valid_loss_ave:0.30668156413013675, acc:0.8817734974424553\n",
      "-------------------------------------------------------------------\n",
      "epoch:43, idx:0, batch_sum_loss:27.14862823486328\n",
      "epoch:43, idx:500, batch_sum_loss:28.400291442871094\n",
      "epoch:43, idx:1000, batch_sum_loss:43.131019592285156\n",
      "epoch:43, idx:1500, batch_sum_loss:17.53126335144043\n",
      "epoch:43, idx:2000, batch_sum_loss:40.34149932861328\n",
      "epoch:43, idx:2500, batch_sum_loss:33.355506896972656\n",
      "epoch:43, idx:3000, batch_sum_loss:27.03386878967285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4b35486926ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_ensem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_ensem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ensem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ens_ps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_save_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8872\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-b346cd0a0da0>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, loss_fn, optimizer, device, data_loader, start_epoch, epoches, model_name, start_save_acc)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch:{epoch}, idx:{idx}, batch_sum_loss:{loss.numpy()[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/decorator.py:decorator-gen-238>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_graph)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    224\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_graph)\u001b[0m\n\u001b[1;32m    175\u001b[0m                                           retain_graph)\n\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_param = paddle.load(\"./work/ens_ps_acc_epoch33_8872.pdparams\")\r\n",
    "model_ensem.set_state_dict(model_param)\r\n",
    "optimizer = optim.Adam(parameters=model_ensem.parameters(), learning_rate=0.00001, weight_decay=0.01)\r\n",
    "loss_list, acc_list = trainModel(model_ensem, loss_fn, optimizer, device, train_loader, start_epoch=33, epoches=20, model_name=\"ens_ps\", start_save_acc=0.8872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./work/loss_list.pkl\", \"wb\") as f:\r\n",
    "    pickle.dump(loss_list, f)\r\n",
    "\r\n",
    "with open(\"./work/acc_list.pkl\", \"wb\") as f:\r\n",
    "    pickle.dump(acc_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 绘制损失及ACC变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(obj, collections.Iterator):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return list(data) if isinstance(data, collections.MappingView) else data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\r\n",
    "plt.plot(loss_list)\r\n",
    "plt.show()\r\n",
    "plt.plot(acc_list)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 读取测试集数据并进行推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USE_TIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartMem:28.61Mb, EndMem:6.87Mb (76.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "if USE_TIME:\r\n",
    "    test = pd.read_csv(DATA_PATH + \"lbe_raw_test_with_time.csv\")\r\n",
    "else:\r\n",
    "    test = pd.read_csv(DATA_PATH + \"lbe_raw_test.csv\")\r\n",
    "test = reduce_mem_usage(test, use_uint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = test[feat_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data):\r\n",
    "    data = paddle.to_tensor(data)\r\n",
    "    y_pre = model(data)\r\n",
    "    y_pre = y_pre.squeeze(-1)\r\n",
    "    return y_pre.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getYtest(model, data):\r\n",
    "    # 分割一下data\r\n",
    "    data_list = []\r\n",
    "    # 每块大小变更为batch_size, 然后统一装到list中\r\n",
    "    i = 0\r\n",
    "    while i <= data.shape[0]:\r\n",
    "        data_list.append(data[i:i+BATCH_SIZE])\r\n",
    "        i += BATCH_SIZE\r\n",
    "    # 按块预测，然后再拼接结果\r\n",
    "    y_test_pre = predict(model, data_list[0])\r\n",
    "    for d in data_list[1:]:\r\n",
    "        y_test_pre = np.append(y_test_pre, predict(model, d))\r\n",
    "\r\n",
    "    #y_test_pre = predict(model, data)\r\n",
    "    y_test_label = list(map(lambda x: 1 if x >= 0.5 else 0, y_test_pre))\r\n",
    "    return y_test_pre, y_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_param = paddle.load(\"./work/ens_pds_loss_epoch17.pdparams\")\r\n",
    "model_ensem.set_state_dict(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_param = paddle.load(\"./work/smb_loss_epoch11.pdparams\")\r\n",
    "model_smb.set_state_dict(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pre, y_test_label = getYtest(model_ensem, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pre, y_test_label = getYtest(model_smb, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46069333333333334"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_label) / len(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46314"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_label) / len(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 把模型预测输出存储至文件\r\n",
    "with open(\"./work/pmb_8871_pred.pkl\", \"wb\") as f:\r\n",
    "    pickle.dump(y_test_pre, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 融合SMB、PMB和DEEPFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mergePredVals(pred_list, weight_list):\r\n",
    "    data_nums = len(pred_list[0])\r\n",
    "    model_nums = len(pred_list)\r\n",
    "    merge_vals = []\r\n",
    "    for i in range(data_nums):\r\n",
    "        merge_val = 0\r\n",
    "        for j in range(model_nums):\r\n",
    "            merge_val += weight_list[j] * pred_list[j][i]\r\n",
    "        merge_vals.append(merge_val)\r\n",
    "    res_label = list(map(lambda x: 1 if x >= 0.5 else 0, merge_vals))\r\n",
    "    return res_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取pmb预测\r\n",
    "with open(\"./work/pmb_8867_pred.pkl\", \"rb\") as f:\r\n",
    "    pmb_test_pre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取dfm的预测\r\n",
    "with open(\"./work/dfm_88702_pred.pkl\", \"rb\") as f:\r\n",
    "    dfm_test_pre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list = []\r\n",
    "pred_list.append(pmb_test_pre)\r\n",
    "#pred_list.append(y_test_pre)\r\n",
    "pred_list.append(dfm_test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_list = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_label = mergePredVals(pred_list, weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46667333333333333"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(merge_label) / len(merge_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSubmission(df, y_test_label, save_path):\r\n",
    "    df[\"label\"] = y_test_label\r\n",
    "    submission = df[[\"sid\", \"label\"]]\r\n",
    "    submission.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#getSubmission(test, merge_label, \"./work/submission_merge2.csv\")\r\n",
    "getSubmission(test, y_test_label, \"./work/ens_ps_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09149647, 0.7825435 , 0.01575657, ..., 0.9732083 , 0.9520463 ,\n",
       "       0.9551207 ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Tensor(shape=[1, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
       "       [[0.38271877]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ensem.model_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dfm = DeepFM(cardi_dict=cardi_dict, emb_dim=64, dnn_list=[512, 256, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFM(\n",
      "  (emb_layers): LayerList(\n",
      "    (0): Embedding(467958, 64, sparse=False)\n",
      "    (1): Embedding(292, 64, sparse=False)\n",
      "    (2): Embedding(2102, 64, sparse=False)\n",
      "    (3): Embedding(89, 64, sparse=False)\n",
      "    (4): Embedding(23, 64, sparse=False)\n",
      "    (5): Embedding(509473, 64, sparse=False)\n",
      "    (6): Embedding(6147, 64, sparse=False)\n",
      "    (7): Embedding(58, 64, sparse=False)\n",
      "    (8): Embedding(332, 64, sparse=False)\n",
      "    (9): Embedding(5, 64, sparse=False)\n",
      "    (10): Embedding(864, 64, sparse=False)\n",
      "    (11): Embedding(382, 64, sparse=False)\n",
      "    (12): Embedding(105, 64, sparse=False)\n",
      "    (13): Embedding(25, 64, sparse=False)\n",
      "    (14): Embedding(8, 64, sparse=False)\n",
      "    (15): Embedding(165, 64, sparse=False)\n",
      "    (16): Embedding(2, 64, sparse=False)\n",
      "    (17): Embedding(8, 64, sparse=False)\n",
      "    (18): Embedding(7, 64, sparse=False)\n",
      "    (19): Embedding(24, 64, sparse=False)\n",
      "    (20): Embedding(60, 64, sparse=False)\n",
      "    (21): Embedding(60, 64, sparse=False)\n",
      "  )\n",
      "  (cross_w_list): ParameterList()\n",
      "  (mlp): MLP(\n",
      "    (fc_layers): LayerList(\n",
      "      (0): Linear(in_features=1408, out_features=512, dtype=float32)\n",
      "      (1): Linear(in_features=512, out_features=256, dtype=float32)\n",
      "    )\n",
      "  )\n",
      "  (sum_w_list): ParameterList()\n",
      "  (fc): Linear(in_features=320, out_features=1, dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_dfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
